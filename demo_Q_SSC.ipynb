{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96dcbca-c8de-458b-b0ad-8b11a600532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to data/Discharge/processed.npz. Shapes: x=(339, 16, 20, 1), mask=(339, 16, 20, 1), adj=(20, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "def normalize_station_id(x):\n",
    "    try:\n",
    "        return str(int(x))\n",
    "    except:\n",
    "        return str(x).strip()\n",
    "\n",
    "def make_windows(X, M, window=16, stride=8):\n",
    "    # X: (N, L), M: (N, L)\n",
    "    N, L = X.shape\n",
    "    windows = []\n",
    "    masks = []\n",
    "    for start in range(0, L - window + 1, stride):\n",
    "        w = X[:, start:start+window]        # (N, window)\n",
    "        m = M[:, start:start+window]\n",
    "        # transpose to (window, N)\n",
    "        windows.append(w.T[..., np.newaxis])  # (window, N, 1)\n",
    "        masks.append(m.T[..., np.newaxis])\n",
    "    if len(windows) == 0:\n",
    "        return np.zeros((0, window, N, 1), dtype=np.float32), np.zeros((0, window, N, 1), dtype=np.float32)\n",
    "    Xb = np.stack(windows, axis=0).astype(np.float32)  # (B, T, N, 1)\n",
    "    Mb = np.stack(masks, axis=0).astype(np.float32)\n",
    "    return Xb, Mb\n",
    "\n",
    "def main(data_dir=\"data/Discharge\", window=16, stride=8, save_to=\"data/Discharge/processed.npz\"):\n",
    "    base = Path(data_dir)\n",
    "    src = pd.read_csv(base / \"SSC_discharge.csv\", parse_dates=[\"date\"]).set_index(\"date\")\n",
    "    tgt = pd.read_csv(base / \"SSC_pooled.csv\", parse_dates=[\"date\"]).set_index(\"date\")\n",
    "    flow = pd.read_csv(base / \"SSC_sites_flow_direction.csv\", index_col=0)\n",
    "\n",
    "    # normalize station names\n",
    "    stations = [normalize_station_id(c) for c in src.columns if c != \"date\"]\n",
    "    src = src.loc[:, src.columns]  # keep original col order\n",
    "    src.columns = stations\n",
    "    tgt.columns = [normalize_station_id(c) for c in tgt.columns]\n",
    "    flow.index = flow.index.map(normalize_station_id)\n",
    "    flow.columns = flow.columns.map(normalize_station_id)\n",
    "\n",
    "    # intersect dates\n",
    "    common = src.index.intersection(tgt.index).sort_values()\n",
    "    src = src.loc[common]\n",
    "    tgt = tgt.loc[common]\n",
    "\n",
    "    # Ensure flow contains all stations: reorder flow to the station order\n",
    "    missing = [s for s in stations if s not in flow.index]\n",
    "    if missing:\n",
    "        print(\"WARNING: the following stations are missing in flow matrix:\", missing)\n",
    "    # Reindex (if index missing, rows/cols will be NaN -> fill zeros)\n",
    "    flow = flow.reindex(index=stations, columns=stations, fill_value=0)\n",
    "    adj = flow.values.astype(np.float32)\n",
    "\n",
    "    # build arrays (N, L)\n",
    "    X_source = src.values.T.astype(np.float64)  # (N, L)\n",
    "    X_target_raw = tgt.values.T  # (N, L) contains NaNs\n",
    "\n",
    "    mask_target = (~pd.DataFrame(X_target_raw).isna()).astype(float).values  # (N, L)\n",
    "    X_target_filled = np.nan_to_num(X_target_raw, nan=0.0).astype(np.float64)\n",
    "\n",
    "    # normalize per-station using source (dense)\n",
    "    station_mean = np.nanmean(X_source, axis=1, keepdims=True)\n",
    "    station_std = np.nanstd(X_source, axis=1, keepdims=True) + 1e-6\n",
    "\n",
    "    X_target_norm = (X_target_filled - station_mean) / station_std\n",
    "\n",
    "    # make windows\n",
    "    Xb, Mb = make_windows(X_target_norm, mask_target, window=window, stride=stride)\n",
    "    # Save meta and arrays\n",
    "    np.savez_compressed(save_to,\n",
    "                        x=Xb.astype(np.float32),\n",
    "                        mask=Mb.astype(np.float32),\n",
    "                        adj=adj.astype(np.float32),\n",
    "                        station_ids=np.array(stations),\n",
    "                        mean=station_mean.astype(np.float32).squeeze(),\n",
    "                        std=station_std.astype(np.float32).squeeze())\n",
    "    print(f\"Saved processed data to {save_to}. Shapes: x={Xb.shape}, mask={Mb.shape}, adj={adj.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", default=\"data/Discharge\")\n",
    "    parser.add_argument(\"--window\", type=int, default=16)\n",
    "    parser.add_argument(\"--stride\", type=int, default=8)\n",
    "    parser.add_argument(\"--out\", default=\"data/Discharge/processed.npz\")\n",
    "    args = parser.parse_args()\n",
    "    main(data_dir=args.data_dir, window=args.window, stride=args.stride, save_to=args.out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957c621b-ee2b-45ec-be08-d65c93ad2411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 16, 20, 1) (339, 16, 20, 1) (20, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"data/Discharge/processed.npz\")\n",
    "x, mask, adj = data[\"x\"], data[\"mask\"], data[\"adj\"]\n",
    "print(x.shape, mask.shape, adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b06705b-b4d7-49b8-a265-cca94cc379d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 82.578490\n",
      "Epoch 2, Loss = 64.579495\n",
      "Epoch 3, Loss = 69.506373\n",
      "Epoch 4, Loss = 60.363521\n",
      "Epoch 5, Loss = 70.075543\n",
      "Epoch 6, Loss = 61.893148\n",
      "Epoch 7, Loss = 64.442609\n",
      "Epoch 8, Loss = 60.818612\n",
      "Epoch 9, Loss = 64.965758\n",
      "Epoch 10, Loss = 63.308463\n",
      "Epoch 11, Loss = 66.587658\n",
      "Epoch 12, Loss = 67.658133\n",
      "Epoch 13, Loss = 69.100616\n",
      "Epoch 14, Loss = 49.487439\n",
      "Epoch 15, Loss = 59.068139\n",
      "Epoch 16, Loss = 62.477311\n",
      "Epoch 17, Loss = 66.249729\n",
      "Epoch 18, Loss = 57.942661\n",
      "Epoch 19, Loss = 59.419797\n",
      "Epoch 20, Loss = 44.194377\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from lib.nn.models.grin import GRINet\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "x_t = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "mask_t = torch.tensor(mask, dtype=torch.bool).to(device)\n",
    "adj_t = torch.tensor(adj, dtype=torch.float32).to(device)\n",
    "\n",
    "B, T, N, C = x_t.shape\n",
    "\n",
    "model = GRINet(\n",
    "    adj=adj_t.numpy(),\n",
    "    d_in=1,\n",
    "    d_hidden=64,\n",
    "    d_ff=64,\n",
    "    ff_dropout=0.1,\n",
    "    n_layers=1,\n",
    "    kernel_size=2,\n",
    "    decoder_order=1,\n",
    "    impute_only_holes=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.L1Loss(reduction=\"none\")\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(B):\n",
    "        x_i = x_t[i:i+1]\n",
    "        m_i = mask_t[i:i+1]\n",
    "\n",
    "        aug_mask = (torch.rand_like(m_i.float()) < 0.1) & m_i\n",
    "        train_mask = m_i & (~aug_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        imputed, _ = model(x_i, train_mask)\n",
    "\n",
    "        # only evaluate added missing positions\n",
    "        target_mask = (~train_mask) & m_i  # only original observed that we masked\n",
    "        if target_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        loss = (loss_fn(imputed, x_i) * target_mask.float()).sum() / target_mask.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {ep+1}, Loss = {total_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bffa7808-9bc8-43fe-ba94-cc9b4ced02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_output = model(x_t, mask_t)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296a8e69-3b27-46f3-abef-85a9b991d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 2720\n"
     ]
    }
   ],
   "source": [
    "W = final_output.shape[0]  # 339\n",
    "window = final_output.shape[1]  # 16\n",
    "stride = 8\n",
    "\n",
    "L_total = (W - 1) * stride + window\n",
    "print(\"Total length:\", L_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846d347c-f45d-497f-9744-2046246f11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_windows(imputed, L_total, window=16, stride=8):\n",
    "    # imputed: (B, window, N, 1)\n",
    "    B, w, N, _ = imputed.shape\n",
    "    full = np.zeros((L_total, N))\n",
    "    count = np.zeros((L_total, N))\n",
    "\n",
    "    pos = 0\n",
    "    for i in range(B):\n",
    "        end = min(pos + window, L_total)\n",
    "        take = end - pos\n",
    "        full[pos:end] += imputed[i, :take, :, 0]\n",
    "        count[pos:end] += 1\n",
    "        pos += stride\n",
    "\n",
    "    full = full / np.maximum(count, 1)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3e7e5b-aeb2-49ab-a4c3-8b053f8ce7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (2720, 20)\n"
     ]
    }
   ],
   "source": [
    "merged_norm = merge_windows(final_output.numpy(), L_total)\n",
    "print(\"Merged shape:\", merged_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1ad4d7-9676-463a-9aec-9bc4d424cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data/Discharge/processed.npz\")\n",
    "mean = data[\"mean\"]  # shape (20,)\n",
    "std = data[\"std\"]\n",
    "merged_real = merged_norm * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718e9d28-af6a-4e8f-ace3-d529a670b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = pd.read_csv(\"data/Discharge/SSC_pooled.csv\", parse_dates=[\"date\"]).set_index(\"date\")\n",
    "L_final = merged_real.shape[0]\n",
    "tgt = tgt.iloc[:L_final]\n",
    "\n",
    "target_array = tgt.values\n",
    "original_missing = np.isnan(target_array)\n",
    "valid_mask = ~original_missing\n",
    "\n",
    "np.random.seed(42)\n",
    "synthetic_mask = valid_mask & (np.random.rand(*valid_mask.shape) < 0.2)\n",
    "\n",
    "y_true = target_array[synthetic_mask]\n",
    "y_pred = merged_real[synthetic_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "349c85a4-66a8-4a92-879f-cd6dfc644c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 5.997624942979036e-05\n",
      "RMSE = 1.92834783018547e-08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred)\n",
    "print(\"MAE =\", mae)\n",
    "print(\"RMSE =\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6c75d-01dc-4116-b669-5f0bd79e681e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
